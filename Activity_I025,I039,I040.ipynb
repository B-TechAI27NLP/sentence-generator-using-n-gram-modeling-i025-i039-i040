{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d961ef5",
        "outputId": "f731b2e8-682a-4a9e-ec21-b7f895c6c641"
      },
      "source": [
        "import nltk('all')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(num_sentences=50000):\n",
        "\n",
        "    print(\"Step 1: Loading dataset from Hugging Face... \")\n",
        "    # Using a subset of the Wikipedia dataset\n",
        "    wiki_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split='train') # Load the wikitext dataset\n",
        "\n",
        "    print(f\"Step 2: Processing {num_sentences} sentences...\")\n",
        "    sentences = []\n",
        "    count = 0\n",
        "    # Explicitly load the punkt tokenizer\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') # Load the punkt tokenizer\n",
        "    for paragraph in wiki_dataset: # Iterate through paragraphs in the dataset\n",
        "        if count >= num_sentences: # Check if the desired number of sentences has been reached\n",
        "            break\n",
        "        text = paragraph['text'].strip() # Get the text of the paragraph and remove leading/trailing whitespace\n",
        "        # Ignore empty lines and document headers\n",
        "        if text and not text.startswith('='): # Check if the text is not empty and not a header\n",
        "            sents_in_paragraph = tokenizer.tokenize(text) # Tokenize the paragraph into sentences\n",
        "            for sent in sents_in_paragraph: # Iterate through sentences in the paragraph\n",
        "                if count >= num_sentences: # Check if the desired number of sentences has been reached\n",
        "                    break\n",
        "                # Add start/end tokens and lowercase.\n",
        "                # Three start tokens are added to support up to a 4-gram model.\n",
        "                tokenized_sent = [\"<s>\", \"<s>\", \"<s>\"] + nltk.word_tokenize(sent.lower()) + [\"</s>\"] # Tokenize the sentence, convert to lowercase, and add start/end tokens\n",
        "                sentences.append(tokenized_sent) # Add the processed sentence to the list\n",
        "                count += 1 # Increment the sentence count\n",
        "\n",
        "    print(f\" Successfully processed {len(sentences)} sentences.\")\n",
        "    return sentences # Return the list of processed sentences\n",
        "\n",
        "\n",
        "def train_ngram_model(sentences, n, vocab_size):\n",
        "\n",
        "    model = defaultdict(lambda: defaultdict(lambda: 0)) # Initialize the model with default dictionaries for counts\n",
        "\n",
        "    print(f\"Step 3: Training {n}-gram model...\")\n",
        "    # Count frequencies of n-grams\n",
        "    for sent in sentences: # Iterate through each sentence\n",
        "        for i in range(len(sent) - n + 1): # Iterate through the sentence to get n-grams\n",
        "            context = tuple(sent[i : i + n - 1]) # Get the context (n-1 words before the target)\n",
        "            target = sent[i + n - 1] # Get the target word\n",
        "            model[context][target] += 1 # Increment the count for the n-gram (context, target)\n",
        "\n",
        "    # Calculate probabilities with Laplace Smoothing\n",
        "    for context in model: # Iterate through each context in the model\n",
        "        total_count = float(sum(model[context].values())) # Calculate the total count for the context\n",
        "        # The denominator is increased by the vocabulary size for smoothing\n",
        "        denominator = total_count + vocab_size # Calculate the denominator for probability calculation with smoothing\n",
        "        for target in model[context]: # Iterate through each target word for the context\n",
        "            model[context][target] = (model[context][target] + 1) / denominator # Calculate the smoothed probability of the target word given the context\n",
        "\n",
        "    return model # Return the trained n-gram model\n",
        "\n",
        "def generate_sentence(model, n, start_prompt, max_len=12):\n",
        "\n",
        "    sentence = start_prompt[:] # Initialize the sentence with the start prompt\n",
        "\n",
        "    # Pad the beginning to establish the initial context\n",
        "    context_tokens = [\"<s>\"] * (n - 1) + sentence # Create initial context with start tokens\n",
        "\n",
        "    while len(sentence) < max_len: # Continue generating until max length is reached\n",
        "        # Determine the current context (the last n-1 words)\n",
        "        context = tuple(context_tokens[-(n-1):]) # Get the current context (last n-1 tokens)\n",
        "\n",
        "        # If the context is unknown, we cannot continue.\n",
        "        if context not in model: # Check if the context exists in the model\n",
        "            break # Stop if context is unknown\n",
        "\n",
        "        next_word_dist = model[context] # Get the probability distribution for the next word given the context\n",
        "        if not next_word_dist: # Check if there are any possible next words for the context\n",
        "            break # Stop if no next words are possible\n",
        "\n",
        "        words = list(next_word_dist.keys()) # Get the list of possible next words\n",
        "        probabilities = list(next_word_dist.values()) # Get the list of corresponding probabilities\n",
        "\n",
        "        # Choose the next word based on its probability distribution\n",
        "        next_word = random.choices(words, weights=probabilities, k=1)[0] # Select the next word randomly based on its probability\n",
        "\n",
        "        # Stop if an end-of-sentence token is generated\n",
        "        if next_word == \"</s>\": # Check if the generated word is the end-of-sentence token\n",
        "            break # Stop if end token is generated\n",
        "\n",
        "        sentence.append(next_word) # Add the generated word to the sentence\n",
        "        context_tokens.append(next_word) # Add the generated word to the context tokens\n",
        "\n",
        "    return \" \".join(sentence) # Join the tokens to form a sentence string\n",
        "\n",
        "\n",
        "def calculate_perplexity(model, n, test_sentences, vocab_size, unknown_prob):\n",
        "\n",
        "    total_log_prob = 0 # Initialize total log probability\n",
        "    word_count = 0 # Initialize word count\n",
        "\n",
        "    for sent in test_sentences: # Iterate through each sentence in the test set\n",
        "        # Do not count the start padding tokens in the total word count\n",
        "        word_count += len(sent) - (n - 1) # Update word count, excluding start padding tokens\n",
        "        for i in range(len(sent) - n + 1): # Iterate through the sentence to get n-grams\n",
        "            context = tuple(sent[i : i + n - 1]) # Get the context\n",
        "            target = sent[i + n - 1] # Get the target word\n",
        "\n",
        "            # Get the probability of the target word given the context\n",
        "            context_dist = model.get(context) # Get the distribution for the context\n",
        "            if context_dist: # Check if the context exists in the model\n",
        "                # Use the calculated probability, or a small default if target is new\n",
        "                prob = context_dist.get(target, unknown_prob) # Get the probability of the target word given the context, using unknown_prob for unseen words\n",
        "            else:\n",
        "                # If the entire context is new, use the default probability\n",
        "                prob = unknown_prob # Use unknown probability if the entire context is new\n",
        "\n",
        "            if prob > 0: # Ensure probability is greater than 0 for log calculation\n",
        "                total_log_prob += math.log2(prob) # Add the log probability of the word to the total\n",
        "\n",
        "    # Perplexity is 2 raised to the power of the negative average log probability\n",
        "    cross_entropy = -total_log_prob / word_count # Calculate cross-entropy (average negative log probability)\n",
        "    perplexity = math.pow(2, cross_entropy) # Calculate perplexity (2 to the power of cross-entropy)\n",
        "    return perplexity # Return the calculated perplexity\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Data Preparation ---\n",
        "    all_sentences = load_and_preprocess_data(num_sentences=50000) # Load and preprocess data\n",
        "    random.shuffle(all_sentences) # Shuffle the sentences randomly\n",
        "    # Split data into 90% training and 10% testing\n",
        "    train_size = int(len(all_sentences) * 0.9) # Calculate the size of the training set\n",
        "    train_sentences = all_sentences[:train_size] # Split the data into training set\n",
        "    test_sentences = all_sentences[train_size:] # Split the data into testing set\n",
        "\n",
        "    print(f\"\\nTraining on {len(train_sentences)} sentences, testing on {len(test_sentences)}.\")\n",
        "\n",
        "    # Build vocabulary from the training set only to avoid data leakage\n",
        "    all_words = [word for sent in train_sentences for word in sent] # Flatten the list of training sentences into a list of words\n",
        "    vocab = set(all_words) # Create a set of unique words (vocabulary) from the training data\n",
        "    vocab_size = len(vocab) # Get the size of the vocabulary\n",
        "    print(f\"Vocabulary size: {vocab_size} unique words.\")\n",
        "\n",
        "    # --- Train All Models ---\n",
        "    bigram_model = train_ngram_model(train_sentences, n=2, vocab_size=vocab_size) # Train the bigram model (n=2)\n",
        "    trigram_model = train_ngram_model(train_sentences, n=3, vocab_size=vocab_size) # Train the trigram model (n=3)\n",
        "    fourgram_model = train_ngram_model(train_sentences, n=4, vocab_size=vocab_size) # Train the 4-gram model (n=4)\n",
        "\n",
        "\n",
        "    # --- Generate Sentences (Qualitative Evaluation) ---\n",
        "    start_prompt = [\"the\", \"man\"] # Define the starting prompt for sentence generation\n",
        "    print(f\"\\n--- Generating Sentences (start: '{' '.join(start_prompt)}') ---\")\n",
        "    print(\"This fulfills the qualitative evaluation task of comparing fluency.\")\n",
        "\n",
        "    print(\"\\n## Bigram Model (n=2):\")\n",
        "    for i in range(5): # Generate 5 sentences using the bigram model\n",
        "        print(f\"{i+1}: {generate_sentence(bigram_model, 2, start_prompt)}\")\n",
        "\n",
        "    print(\"\\n## Trigram Model (n=3):\")\n",
        "    for i in range(5): # Generate 5 sentences using the trigram model\n",
        "        print(f\"{i+1}: {generate_sentence(trigram_model, 3, start_prompt)}\")\n",
        "\n",
        "    print(\"\\n## 4-gram Model (n=4):\")\n",
        "    for i in range(5): # Generate 5 sentences using the 4-gram model\n",
        "        print(f\"{i+1}: {generate_sentence(fourgram_model, 4, start_prompt)}\")\n",
        "\n",
        "    # --- Calculate Perplexity (Quantitative Evaluation) ---\n",
        "    print(\"\\n--- Calculating Perplexity on Test Set ---\")\n",
        "\n",
        "    # Base probability for unseen n-grams after smoothing\n",
        "    unknown_prob = 1 / vocab_size # Calculate the probability for unknown words with Laplace smoothing\n",
        "\n",
        "    bi_perplexity = calculate_perplexity(bigram_model, 2, test_sentences, vocab_size, unknown_prob) # Calculate perplexity for bigram model\n",
        "    print(f\"Bigram Model Perplexity: {bi_perplexity:.2f}\")\n",
        "\n",
        "    tri_perplexity = calculate_perplexity(trigram_model, 3, test_sentences, vocab_size, unknown_prob) # Calculate perplexity for trigram model\n",
        "    print(f\"Trigram Model Perplexity: {tri_perplexity:.2f}\")\n",
        "    four_perplexity = calculate_perplexity(fourgram_model, 4, test_sentences, vocab_size, unknown_prob) # Calculate perplexity for 4-gram model\n",
        "    print(f\"4-gram Model Perplexity: {four_perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTVtU7zzriMs",
        "outputId": "31a7a5e3-ec5c-45bb-a637-3ff79cd394e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Loading dataset from Hugging Face... \n",
            "Step 2: Processing 50000 sentences...\n",
            " Successfully processed 50000 sentences.\n",
            "\n",
            "Training on 45000 sentences, testing on 5000.\n",
            "Vocabulary size: 48391 unique words.\n",
            "Step 3: Training 2-gram model...\n",
            "Step 3: Training 3-gram model...\n",
            "Step 3: Training 4-gram model...\n",
            "\n",
            "--- Generating Sentences (start: 'the man') ---\n",
            "This fulfills the qualitative evaluation task of comparing fluency.\n",
            "\n",
            "## Bigram Model (n=2):\n",
            "1: the man comic role for 198 minutes and blues asked her type\n",
            "2: the man 10 miles before redoing segments of a variety of controlled\n",
            "3: the man , vuthipong demanded that thin and in the foppish tattle\n",
            "4: the man named `` , according to solely producing video transmissions of\n",
            "5: the man theory .\n",
            "\n",
            "## Trigram Model (n=3):\n",
            "1: the man of genius though he admitted the team .\n",
            "2: the man who has attempted to recruit mainly in eastern south africa\n",
            "3: the man 's ideas for the trinity mirror group , in order\n",
            "4: the man is an american singer gwen stefani , to release players\n",
            "5: the man in white is always greater â€” by now moved )\n",
            "\n",
            "## 4-gram Model (n=4):\n",
            "1: the man of metropolis steals our hearts `` , `` opportunities (\n",
            "2: the man selected to lead another expedition for a missing aircraft near\n",
            "3: the man selected to lead another expedition for a missing aircraft near\n",
            "4: the man of metropolis steals our hearts `` , which was going\n",
            "5: the man selected to lead the commando force was lieutenant colonel charles\n",
            "\n",
            "--- Calculating Perplexity on Test Set ---\n",
            "Bigram Model Perplexity: 2114.18\n",
            "Trigram Model Perplexity: 13735.35\n",
            "4-gram Model Perplexity: 30472.78\n"
          ]
        }
      ]
    }
  ]
}